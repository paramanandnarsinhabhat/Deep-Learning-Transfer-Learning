{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deccd076-678e-4a37-8831-29d29c216062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning \n",
    "## Classification Problem - Emergency vs Non-emergency Vehicle Classification\n",
    "\n",
    "## 1. Import neccessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "#used to preprocess data according to VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "#for instantiating the model and loading the weights and biases\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6d9ec2-1492-4f45-bf41-5b47c753b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  image_names  emergency_or_not\n",
      "0       0.jpg                 1\n",
      "1       1.jpg                 1\n",
      "2       2.jpg                 1\n",
      "3       3.jpg                 1\n",
      "4       4.jpg                 1\n"
     ]
    }
   ],
   "source": [
    "## 2. Load the data\n",
    "#reading the csv file containing data labels\n",
    "data = pd.read_csv('../data/emergency_classification.csv')\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70150503-bd0a-4fc8-9424-56b5368ab3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty python list\n",
    "X = []\n",
    "\n",
    "# go through all the image locations one by one\n",
    "for img_name in data.image_names:\n",
    "    # read the image from location\n",
    "    img = plt.imread('../data/images/' + img_name)\n",
    "    # pile it one over the other\n",
    "    X.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427f8380-ed6a-466c-b99a-c59ed7a6c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this python list to a single numpy array\n",
    "X = np.array(X)\n",
    "\n",
    "#getting the labels for images\n",
    "y = data.emergency_or_not.values\n",
    "#converting label to categorical i.e instead of 0/1 labels we have 2 columns emergency and non-emergency ,\n",
    "#with only one of them is true for every image\n",
    "y = to_categorical(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1052802b-5fe9-45ab-98b8-781fc608190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255\n"
     ]
    }
   ],
   "source": [
    "## 3. Pre-Process Data\n",
    "'''\n",
    "Steps : \n",
    "1. Pre-process the data as per model's requirement\n",
    "2. Prepare training and validation set\n",
    "'''\n",
    "#show maximum and minimum values for the image array\n",
    "X.min(), X.max()\n",
    "\n",
    "print(X.min(), X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79bfb790-4903-4e97-9936-ffec59526fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-123.68 151.061\n"
     ]
    }
   ],
   "source": [
    "#preprocess input images accordiing to requirements of VGG16 model\n",
    "X = preprocess_input(X, data_format=None)\n",
    "\n",
    "#print minimum and maximum values present in the array\n",
    "X.min(), X.max()\n",
    "\n",
    "print(X.min(), X.max()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ac4178-42e2-4ad2-aa74-8a6ed4d75e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-233.558 27.381004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 00:09:32.551613: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-02-05 00:09:32.551721: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-02-05 00:09:32.551761: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-02-05 00:09:32.553309: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-05 00:09:32.557231: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138357544 (527.79 MB)\n",
      "Trainable params: 138357544 (527.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#preprocess input images accordiing to requirements of VGG16 model\n",
    "X = preprocess_input(X, data_format=None)\n",
    "\n",
    "#print minimum and maximum values present in the array\n",
    "X.min(), X.max()\n",
    "\n",
    "print(X.min(), X.max()\n",
    ")\n",
    "# splitting the dataset into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "## 4. Load weights of pretrained model\n",
    "# creating model with pre trained imagenet weights\n",
    "base_model = VGG16(weights='imagenet')\n",
    "\n",
    "#shows model summary\n",
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da80031-8783-439b-9cea-7d6c9382f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating a VGG16 model with imagenet pretrained weights , accepting input of shape (224,224,3)\n",
    "# also remove the final layers from model(include_top= False)\n",
    "base_model = VGG16(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n",
    "\n",
    "# show model summary\n",
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1f1f4d-2725-4161-9bfc-267f6a7c36ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 00:10:19.850871: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 60s 1s/step\n",
      "23/23 [==============================] - 25s 1s/step\n",
      "(1646, 7, 7, 512)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 41294848 into shape (1152,25088)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(base_model_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# flattening the model output to one dimension for every sample of training set\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m base_model_pred \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1152\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(base_model_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(base_model_pred_valid\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 41294848 into shape (1152,25088)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## 5. Fine tune the model for the current problem\n",
    "Steps:-\n",
    "1. Extract features\n",
    "2. Flatten the data\n",
    "3. Rescale features\n",
    "4. Create a Neural Network Model\n",
    "5. Compile the model\n",
    "6. Train and Validate the model\n",
    "'''\n",
    "# extract features using the pretrained VGG16 model\n",
    "# for training set\n",
    "base_model_pred = base_model.predict(X_train)\n",
    "#for validation set\n",
    "base_model_pred_valid = base_model.predict(X_valid)\n",
    "\n",
    "#show shape of predictions\n",
    "print(base_model_pred.shape)\n",
    "\n",
    "\n",
    "\n",
    "total_elements = base_model_pred.size  # This should be 41294848 in your case\n",
    "new_row_count = 1152\n",
    "new_col_count = total_elements // new_row_count  # Use floor division to get an integer result\n",
    "\n",
    "base_model_pred = base_model_pred.reshape(new_row_count, new_col_count)\n",
    "\n",
    "\n",
    "\n",
    "# flattening the model output to one dimension for every sample of training set\n",
    "base_model_pred = base_model_pred.reshape(1152, 7*7*512)\n",
    "\n",
    "print(base_model_pred.shape)\n",
    "\n",
    "print(base_model_pred_valid.shape)\n",
    "\n",
    "# flattening the model output to one dimension for every sample of validation set\n",
    "base_model_pred_valid = base_model_pred_valid.reshape(494, 7*7*512)\n",
    "\n",
    "print(base_model_pred_valid.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be255ef-5ea8-43d2-bcec-7a16f59ee3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the min and max of the extracted features\n",
    "base_model_pred.min(), base_model_pred.max()\n",
    "\n",
    "#get maximum value from generated features\n",
    "max_val = base_model_pred.max()\n",
    "\n",
    "#normalizing features generated from the VGG16 model to [0,1]\n",
    "base_model_pred = base_model_pred / max_val\n",
    "base_model_pred_valid = base_model_pred_valid / max_val\n",
    "base_model_pred.min(), base_model_pred.max()\n",
    "\n",
    "#create a sequential model \n",
    "model = Sequential()\n",
    "# add input layer to the model that accepts input of shape 7*7*512\n",
    "model.add(InputLayer((7*7*512, )))\n",
    "# add fully connected layer with 1024 neurons and relu activation\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "# add fully connected layer with 2 neurons and relu activation\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='sgd', metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91633d5-7705-4a36-b454-0231d083b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model using features generated from VGG16 model\n",
    "model.fit(base_model_pred, y_train, epochs=100, validation_data=(base_model_pred_valid, y_valid))\n",
    "\n",
    "## 6. Get Predictions\n",
    "# get predictions\n",
    "predictions = model.predict_classes(base_model_pred_valid)\n",
    "#show predictions\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
